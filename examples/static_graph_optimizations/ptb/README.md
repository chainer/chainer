# Recurrent Net Language Model with static subgraph optimization

This is an example of a recurrent net for language modeling.
The network is trained to predict the word given the preceding word sequence.

This example is based on the following RNNLM implementation written in Torch7.
https://github.com/tomsercu/lstm

If you want to run this example on the N-th GPU, pass `--gpu=N` to the script.
