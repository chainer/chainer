{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_en.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1GipLOaiixna3-fW3c2mwkESR-xKkS0mt",
          "timestamp": 1527401219789
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "FQiOKlk03GgY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analisys with Recursive Neural Network\n",
        "Note: This notebook is created from [chainer/examples/sentiment](https://github.com/chainer/chainer/tree/master/examples/sentiment). If you want to run it as script, please refer to the above link.\n",
        "\n",
        "In this notebook, we will analysys the sentiment of the documents by using Recursive Neural Network."
      ]
    },
    {
      "metadata": {
        "id": "UC-m7Hwh4Y7Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, we execute the following cell and install \"Chainer\" and its GPU back end \"CuPy\". If the \"runtime type\" of Colaboratory is GPU, you can run Chainer with GPU as a backend."
      ]
    },
    {
      "metadata": {
        "id": "vctT_8EWSEOJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "outputId": "2d2868ef-7e94-495b-9a04-0b5ec43897f8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527478518526,
          "user_tz": -540,
          "elapsed": 39007,
          "user": {
            "displayName": "Keisuke Umezawa",
            "photoUrl": "//lh3.googleusercontent.com/-KRaJUYLr9rk/AAAAAAAAAAI/AAAAAAAAABE/PM10WWYaRqk/s50-c-k-no/photo.jpg",
            "userId": "105009450683196526456"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!apt -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!pip install -q cupy-cuda80 chainer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libcusparse8.0 libnvrtc8.0 libnvtoolsext1\n",
            "0 upgraded, 3 newly installed, 0 to remove and 0 not upgraded.\n",
            "Need to get 28.9 MB of archives.\n",
            "After this operation, 71.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu artful/multiverse amd64 libcusparse8.0 amd64 8.0.61-1 [22.6 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu artful/multiverse amd64 libnvrtc8.0 amd64 8.0.61-1 [6,225 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu artful/multiverse amd64 libnvtoolsext1 amd64 8.0.61-1 [32.2 kB]\n",
            "Fetched 28.9 MB in 2s (10.4 MB/s)\n",
            "\n",
            "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libcusparse8.0:amd64.\n",
            "(Reading database ... 18298 files and directories currently installed.)\n",
            "Preparing to unpack .../libcusparse8.0_8.0.61-1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libcusparse8.0:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [#######...................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Selecting previously unselected package libnvrtc8.0:amd64.\n",
            "Preparing to unpack .../libnvrtc8.0_8.0.61-1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking libnvrtc8.0:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [##################........................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 37%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package libnvtoolsext1:amd64.\n",
            "Preparing to unpack .../libnvtoolsext1_8.0.61-1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 43%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking libnvtoolsext1:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libnvtoolsext1:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libcusparse8.0:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libnvrtc8.0:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 87%]\u001b[49m\u001b[39m [##################################################........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [######################################################....] \u001b8Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "\n",
            "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cH8mST0B5IK2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's import the necessary modules, then check the version of Chainer, NumPy, CuPy, Cuda and other execution environments."
      ]
    },
    {
      "metadata": {
        "id": "v00bch6E5Gf6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "67762800-a400-49aa-d045-6a3a4eda5b19",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527406727103,
          "user_tz": -540,
          "elapsed": 529,
          "user": {
            "displayName": "Keisuke Umezawa",
            "photoUrl": "//lh3.googleusercontent.com/-KRaJUYLr9rk/AAAAAAAAAAI/AAAAAAAAABE/PM10WWYaRqk/s50-c-k-no/photo.jpg",
            "userId": "105009450683196526456"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "import chainer\n",
        "from chainer import cuda\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "from chainer.training import extensions\n",
        "from chainer import reporter\n",
        "\n",
        "\n",
        "chainer.print_runtime_info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chainer: 4.1.0\n",
            "NumPy: 1.14.3\n",
            "CuPy:\n",
            "  CuPy Version          : 4.1.0\n",
            "  CUDA Root             : None\n",
            "  CUDA Build Version    : 8000\n",
            "  CUDA Driver Version   : 9000\n",
            "  CUDA Runtime Version  : 8000\n",
            "  cuDNN Build Version   : 7102\n",
            "  cuDNN Version         : 7102\n",
            "  NCCL Build Version    : 2104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q735WN-T6C4J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Preparation of training data\n",
        "In this notebook, we will use the training data which are preprocessed by [chainer/examples/sentiment/download.py](https://github.com/chainer/chainer/tree/master/examples/sentiment/download.py). Let's run the following cells, download the necessary training data and unzip it."
      ]
    },
    {
      "metadata": {
        "id": "nZVIeRChTmM5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# download.py\n",
        "import os.path\n",
        "from six.moves.urllib import request\n",
        "import zipfile\n",
        "\n",
        "\n",
        "request.urlretrieve(\n",
        "    'https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip',\n",
        "    'trainDevTestTrees_PTB.zip')\n",
        "zf = zipfile.ZipFile('trainDevTestTrees_PTB.zip')\n",
        "for name in zf.namelist():\n",
        "    (dirname, filename) = os.path.split(name)\n",
        "    if not filename == '':\n",
        "        zf.extract(name, '.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "serSskBV69nB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's execute the following command and check if the training data have been prepared.\n",
        "\n",
        "```\n",
        "dev.txt  test.txt  train.txt\n",
        "```\n",
        "It will be OK if the above output is displayed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ry-1ylFZiMQ9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "260ad7f3-a9eb-48ae-e7c8-1b6e92cdde26",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527406730335,
          "user_tz": -540,
          "elapsed": 1772,
          "user": {
            "displayName": "Keisuke Umezawa",
            "photoUrl": "//lh3.googleusercontent.com/-KRaJUYLr9rk/AAAAAAAAAAI/AAAAAAAAABE/PM10WWYaRqk/s50-c-k-no/photo.jpg",
            "userId": "105009450683196526456"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls trees"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev.txt  test.txt  train.txt\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZOMNLMahSFRv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's look at the first line of ``test.txt`` and see how each sample is written."
      ]
    },
    {
      "metadata": {
        "id": "ob5vJeMNSSmQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4616e71a-d2fe-43a7-9036-1effdca730f2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527406731997,
          "user_tz": -540,
          "elapsed": 1638,
          "user": {
            "displayName": "Keisuke Umezawa",
            "photoUrl": "//lh3.googleusercontent.com/-KRaJUYLr9rk/AAAAAAAAAAI/AAAAAAAAABE/PM10WWYaRqk/s50-c-k-no/photo.jpg",
            "userId": "105009450683196526456"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!head trees/dev.txt -n1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3 (2 It) (4 (4 (2 's) (4 (3 (2 a) (4 (3 lovely) (2 film))) (3 (2 with) (4 (3 (3 lovely) (2 performances)) (2 (2 by) (2 (2 (2 Buy) (2 and)) (2 Accorsi))))))) (2 .)))\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y5Xqp9n7Srjr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As displayed above, each sample is defined by a tree structure.\n",
        "\n",
        "The tree structure is recursively defined as `` (value, node) ``, and the class label for `` node`` is `` value``.\n",
        "\n",
        "The class labels represent 1(really negative), 2(negative), 3(neutral), 4(positive), and 5(really positive), respectively.\n",
        "\n",
        "The representation of the one sample is shown below.\n",
        "\n",
        "<img src=\"http://slideplayer.com/slide/6336754/22/images/4/Sentiment+Analysis+(Socher+et+al,+2013).jpg\" width=\"600\">"
      ]
    },
    {
      "metadata": {
        "id": "kjBUS6Cr8Sye",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Setting parameters\n",
        "Here we set the parameters for training.\n",
        "* `` n_epoch``: Epoch number. How many times we pass through the whole training data.\n",
        "* `` n_units``: Number of units. How many hidden state vectors each Recursive Neural Network node has.\n",
        "* `` batchsize``: Batch size. How many train data we will input as a block when updating parameters.\n",
        "* `` n_label``: Number of labels. Number of classes to be identified. Since there are 5 labels this time, `` 5``.\n",
        "* `` epoch_per_eval``: How often to perform validation.\n",
        "* `` is_test``: If `` True``, we use a small dataset.\n",
        "* `` gpu_id``: GPU ID. The ID of the GPU to use. For Colaboratory it is good to use `` 0``."
      ]
    },
    {
      "metadata": {
        "id": "1Ha6-ramShVF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "n_epoch = 100  # number of epochs\n",
        "n_units = 30  # number of units per layer\n",
        "batchsize = 25  # minibatch size\n",
        "n_label = 5  # number of labels\n",
        "epoch_per_eval = 5  # number of epochs per evaluation\n",
        "is_test = True\n",
        "gpu_id = 0\n",
        "\n",
        "if is_test:\n",
        "    max_size = 10\n",
        "else:\n",
        "    max_size = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q3PnMSM78akB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Preparing the iterator\n",
        "\n",
        "Let's read the dataset used for training, validation, test and create an Iterator."
      ]
    },
    {
      "metadata": {
        "id": "ermPnCx061Hq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, we convert each sample represented by ``str`` type to a tree structure data represented by a ``dictionary`` type.\n",
        "\n",
        "We will tokenize the string with `` read_corpus`` implemented by the parser `` SexpParser``. After that, we convert each tokenized sample to a tree structure data  by `` convert_tree``. By doing like this, it is possible to express a label as ``int``, a node as a two-element ``tuple``, and a tree structure as a ``dictionary``, making it a more manageable data structure than the original string.\n"
      ]
    },
    {
      "metadata": {
        "id": "PqEzpAzMeMVo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# data.py\n",
        "import codecs\n",
        "import re\n",
        "\n",
        "\n",
        "class SexpParser(object):\n",
        "\n",
        "    def __init__(self, line):\n",
        "        self.tokens = re.findall(r'\\(|\\)|[^\\(\\) ]+', line)\n",
        "        self.pos = 0\n",
        "\n",
        "    def parse(self):\n",
        "        assert self.pos < len(self.tokens)\n",
        "        token = self.tokens[self.pos]\n",
        "        assert token != ')'\n",
        "        self.pos += 1\n",
        "\n",
        "        if token == '(':\n",
        "            children = []\n",
        "            while True:\n",
        "                assert self.pos < len(self.tokens)\n",
        "                if self.tokens[self.pos] == ')':\n",
        "                    self.pos += 1\n",
        "                    break\n",
        "                else:\n",
        "                    children.append(self.parse())\n",
        "            return children\n",
        "        else:\n",
        "            return token\n",
        "\n",
        "\n",
        "def read_corpus(path, max_size):\n",
        "    with codecs.open(path, encoding='utf-8') as f:\n",
        "        trees = []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            tree = SexpParser(line).parse()\n",
        "            trees.append(tree)\n",
        "            if max_size and len(trees) >= max_size:\n",
        "                break\n",
        "\n",
        "    return trees\n",
        "\n",
        "  \n",
        "def convert_tree(vocab, exp):\n",
        "    assert isinstance(exp, list) and (len(exp) == 2 or len(exp) == 3)\n",
        "\n",
        "    if len(exp) == 2:\n",
        "        label, leaf = exp\n",
        "        if leaf not in vocab:\n",
        "            vocab[leaf] = len(vocab)\n",
        "        return {'label': int(label), 'node': vocab[leaf]}\n",
        "    elif len(exp) == 3:\n",
        "        label, left, right = exp\n",
        "        node = (convert_tree(vocab, left), convert_tree(vocab, right))\n",
        "        return {'label': int(label), 'node': node}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XrP2AfaqXmpm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's use `` read_corpus () `` and `` convert_tree () `` to create an iterator."
      ]
    },
    {
      "metadata": {
        "id": "42l-fDBfRijd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "    \n",
        "train_data = [convert_tree(vocab, tree) \n",
        "                        for tree in read_corpus('trees/train.txt', max_size)]\n",
        "train_iter = chainer.iterators.SerialIterator(train_data, batchsize)\n",
        "\n",
        "validation_data = [convert_tree(vocab, tree) \n",
        "                                 for tree in read_corpus('trees/dev.txt', max_size)]\n",
        "validation_iter = chainer.iterators.SerialIterator(validation_data, batchsize, \n",
        "                                                                                   repeat=False, shuffle=False)\n",
        "\n",
        "test_data = [convert_tree(vocab, tree) \n",
        "                        for tree in read_corpus('trees/test.txt', max_size)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xE3MSUrj92Tn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's try to display the first element of `` test_data``. It is represented by the following tree structure, `` lable`` expresses the score of that `` node``, and the numerical value of the leaf `` node`` corresponds to the word id in the dictionary `` vocab``."
      ]
    },
    {
      "metadata": {
        "id": "aE4YORp2eQP9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d92b860-bd3f-4715-dea3-a2c9b93a302b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527406734154,
          "user_tz": -540,
          "elapsed": 396,
          "user": {
            "displayName": "Keisuke Umezawa",
            "photoUrl": "//lh3.googleusercontent.com/-KRaJUYLr9rk/AAAAAAAAAAI/AAAAAAAAABE/PM10WWYaRqk/s50-c-k-no/photo.jpg",
            "userId": "105009450683196526456"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(test_data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'label': 2, 'node': ({'label': 3, 'node': ({'label': 3, 'node': 252}, {'label': 2, 'node': 71})}, {'label': 1, 'node': ({'label': 1, 'node': 253}, {'label': 2, 'node': 254})})}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GVUZPtjo8ycv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 4. Preparing the model\n",
        "\n",
        "Let's define the network."
      ]
    },
    {
      "metadata": {
        "id": "eU55nEkoE_Kc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We traverse each node of the tree structure data by `` traverse`` and calculate the loss `` loss`` of the whole tree. The implementation of `` traverse`` is a recursive call, which will traverse child nodes in turn. (It is a common implementation when treating tree structure data!)\n",
        "\n",
        "First, we calculate the hidden state vector `` v``. In the case of a leaf node, we obtain a hidden state vector stored in `` embed`` by `` model.leaf(word) `` from word id`` word``. In the case of an intermediate node, the hidden vector is calculated with the hidden state vector `` left`` and `` right`` of the child nodes by `` v = model.node(left, right)``.\n",
        "\n",
        "`` loss += F.softmax_cross_entropy(y, t) `` adds the loss of the current node to the loss of the child node, then returns loss to the parent node by ``return loss, v``.\n",
        "\n",
        "After the line `` loss += F.softmax_cross_entropy(y, t)``, there are some lines for logging accuracy and etc. But it is not necessary for the model definition itself."
      ]
    },
    {
      "metadata": {
        "id": "aGICf_k0hzW8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class RecursiveNet(chainer.Chain):\n",
        "  \n",
        "    def traverse(self, node, evaluate=None, root=True):\n",
        "        if isinstance(node['node'], int):\n",
        "            # leaf node\n",
        "            word = self.xp.array([node['node']], np.int32)\n",
        "            loss = 0\n",
        "            v = model.leaf(word)\n",
        "        else:\n",
        "            # internal node\n",
        "            left_node, right_node = node['node']\n",
        "            left_loss, left = self.traverse(left_node, evaluate=evaluate, root=False)\n",
        "            right_loss, right = self.traverse(right_node, evaluate=evaluate, root=False)\n",
        "            v = model.node(left, right)\n",
        "            loss = left_loss + right_loss\n",
        "\n",
        "        y = model.label(v)\n",
        "\n",
        "        label = self.xp.array([node['label']], np.int32)\n",
        "        t = chainer.Variable(label)\n",
        "        loss += F.softmax_cross_entropy(y, t)\n",
        "\n",
        "        predict = cuda.to_cpu(y.data.argmax(1))\n",
        "        if predict[0] == node['label']:\n",
        "            evaluate['correct_node'] += 1\n",
        "        evaluate['total_node'] += 1\n",
        "  \n",
        "        if root:\n",
        "            if predict[0] == node['label']:\n",
        "                evaluate['correct_root'] += 1\n",
        "            evaluate['total_root'] += 1\n",
        "\n",
        "        return loss, v\n",
        "\n",
        "    def __init__(self, n_vocab, n_units):\n",
        "        super(RecursiveNet, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.embed = L.EmbedID(n_vocab, n_units)\n",
        "            self.l = L.Linear(n_units * 2, n_units)\n",
        "            self.w = L.Linear(n_units, n_label)\n",
        "\n",
        "    def leaf(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "    def node(self, left, right):\n",
        "        return F.tanh(self.l(F.concat((left, right))))\n",
        "\n",
        "    def label(self, v):\n",
        "        return self.w(v)\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        accum_loss = 0.0\n",
        "        result = collections.defaultdict(lambda: 0)\n",
        "        for tree in x:\n",
        "            loss, _ = self.traverse(tree, evaluate=result)\n",
        "            accum_loss += loss\n",
        "        \n",
        "        reporter.report({'loss': accum_loss}, self)\n",
        "        reporter.report({'total': result['total_node']}, self)\n",
        "        reporter.report({'correct': result['correct_node']}, self)\n",
        "        return accum_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYsIEJUI_Km4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One attention to the implementation of `` __call__``.\n",
        "\n",
        "`` x`` passed to `` __call__`` is mini-batched input data and contains samples `` s_n`` like `` [s_1, s_2, ..., s_N] ``.\n",
        "\n",
        "In a network such as Convolutional Network used for image recognition, it is possible to perform parallel calculation collectively for mini batch `` x``. However, in the case of a tree-structured network like this one, it is difficult to compute parallel because of the following reasons.\n",
        "\n",
        "* Data length varies depending on samples.\n",
        "* The order of calculation for each sample is different.\n",
        "\n",
        "So, the implementation is to calculate each sample and finally summarize the results.\n",
        " \n",
        "Note: Actually, you can perform parallel calculation of mini batch in Recursive Neural Network by using stack. Since it is published in the latter part of notebook as (Advanced), please refer to it.\n"
      ]
    },
    {
      "metadata": {
        "id": "TnBhdpO8igK9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = RecursiveNet(len(vocab), n_units)\n",
        "\n",
        "if gpu_id >= 0:\n",
        "    model.to_gpu()\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = chainer.optimizers.AdaGrad(lr=0.1)\n",
        "optimizer.setup(model)\n",
        "optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(0.0001))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OTq_sX298-ZR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. Preparation and training of Updater Â· Trainer\n",
        "\n",
        "As usual, we define an updater and a trainer to train the model.\n",
        "This time, I do not use `` L.Classifier`` and calculate the accuracy `` accuracy`` by myself. You can easily implement it using `` extensions.MicroAverage``. For details, please refer to [chainer.training.extensions.MicroAverage](https://docs.chainer.org/en/latest/reference/generated/chainer.training.extensions.MicroAverage.html)."
      ]
    },
    {
      "metadata": {
        "id": "nXAIZSy9cQdu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "outputId": "101e9690-7baf-487d-b0bb-ce1e98ffc176",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527406870368,
          "user_tz": -540,
          "elapsed": 108505,
          "user": {
            "displayName": "Keisuke Umezawa",
            "photoUrl": "//lh3.googleusercontent.com/-KRaJUYLr9rk/AAAAAAAAAAI/AAAAAAAAABE/PM10WWYaRqk/s50-c-k-no/photo.jpg",
            "userId": "105009450683196526456"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def _convert(batch, device):\n",
        "  return batch\n",
        "\n",
        "updater = chainer.training.StandardUpdater(\n",
        "    train_iter, optimizer, device=gpu_id, converter=_convert)\n",
        "\n",
        "trainer = chainer.training.Trainer(updater, (n_epoch, 'epoch'))\n",
        "trainer.extend(\n",
        "        extensions.Evaluator(validation_iter, model, device=gpu_id, converter=_convert),\n",
        "        trigger=(epoch_per_eval, 'epoch'))\n",
        "trainer.extend(extensions.LogReport())\n",
        "\n",
        "trainer.extend(extensions.MicroAverage(\n",
        "        'main/correct', 'main/total', 'main/accuracy'))\n",
        "trainer.extend(extensions.MicroAverage(\n",
        "        'validation/main/correct', 'validation/main/total',\n",
        "        'validation/main/accuracy'))\n",
        "\n",
        "trainer.extend(extensions.PrintReport(\n",
        "        ['epoch', 'main/loss', 'validation/main/loss',\n",
        "          'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
        "trainer.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
            "\u001b[J1           1707.8                            0.155405                                 14.6668       \n",
            "\u001b[J2           586.467     556.419               0.497748       0.396175                  17.3996       \n",
            "\u001b[J3           421.267                           0.657658                                 19.3942       \n",
            "\u001b[J4           320.414     628.025               0.772523       0.42623                   22.2462       \n",
            "\u001b[J5           399.621                           0.704955                                 24.208        \n",
            "\u001b[J6           318.544     595.03                0.786036       0.420765                  27.0585       \n",
            "\u001b[J7           231.529                           0.880631                                 29.0178       \n",
            "\u001b[J8           160.546     628.959               0.916667       0.431694                  31.7562       \n",
            "\u001b[J9           122.076                           0.957207                                 33.8269       \n",
            "\u001b[J10          93.6623     669.898               0.975225       0.445355                  36.5802       \n",
            "\u001b[J11          74.2366                           0.986486                                 38.5855       \n",
            "\u001b[J12          60.2297     701.062               0.990991       0.448087                  41.4308       \n",
            "\u001b[J13          49.7152                           0.997748                                 43.414        \n",
            "\u001b[J14          41.633      724.893               0.997748       0.453552                  46.1698       \n",
            "\u001b[J15          35.3564                           0.997748                                 48.1999       \n",
            "\u001b[J16          30.402      744.493               1              0.448087                  50.9842       \n",
            "\u001b[J17          26.4137                           1                                        53.0605       \n",
            "\u001b[J18          23.188      760.43                1              0.459016                  55.7924       \n",
            "\u001b[J19          20.5913                           1                                        57.7479       \n",
            "\u001b[J20          18.4666     773.808               1              0.461749                  60.5636       \n",
            "\u001b[J21          16.698                            1                                        62.52         \n",
            "\u001b[J22          15.2066     785.205               1              0.461749                  65.2603       \n",
            "\u001b[J23          13.9351                           1                                        67.3052       \n",
            "\u001b[J24          12.8404     794.963               1              0.461749                  70.0323       \n",
            "\u001b[J25          11.8897                           1                                        71.9788       \n",
            "\u001b[J26          11.0575     803.388               1              0.459016                  74.7653       \n",
            "\u001b[J27          10.3237                           1                                        76.7485       \n",
            "\u001b[J28          9.67249     810.727               1              0.472678                  79.4539       \n",
            "\u001b[J29          9.09113                           1                                        81.4813       \n",
            "\u001b[J30          8.56935     817.176               1              0.480874                  84.1942       \n",
            "\u001b[J31          8.09874                           1                                        86.2475       \n",
            "\u001b[J32          7.6724      822.889               1              0.480874                  88.956        \n",
            "\u001b[J33          7.2846                            1                                        90.9035       \n",
            "\u001b[J34          6.93052     827.989               1              0.480874                  93.6949       \n",
            "\u001b[J35          6.60615                           1                                        95.6557       \n",
            "\u001b[J36          6.30805     832.574               1              0.486339                  98.4233       \n",
            "\u001b[J37          6.03332                           1                                        100.456       \n",
            "\u001b[J38          5.77941     836.724               1              0.486339                  103.18        \n",
            "\u001b[J39          5.5442                            1                                        105.129       \n",
            "\u001b[J40          5.32575     840.507               1              0.486339                  107.954       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N7wiomoIvjc1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6. Checking the performance with test data"
      ]
    },
    {
      "metadata": {
        "id": "J4g1JkNWv86t",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "63d9353f-04a9-40fe-e673-80592ebde06d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527406871545,
          "user_tz": -540,
          "elapsed": 1089,
          "user": {
            "displayName": "Keisuke Umezawa",
            "photoUrl": "//lh3.googleusercontent.com/-KRaJUYLr9rk/AAAAAAAAAAI/AAAAAAAAABE/PM10WWYaRqk/s50-c-k-no/photo.jpg",
            "userId": "105009450683196526456"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_trees):\n",
        "    result = collections.defaultdict(lambda: 0)\n",
        "    with chainer.using_config('train', False), chainer.no_backprop_mode():\n",
        "        for tree in test_trees:\n",
        "            model.traverse(tree, evaluate=result)\n",
        "    acc_node = 100.0 * result['correct_node'] / result['total_node']\n",
        "    acc_root = 100.0 * result['correct_root'] / result['total_root']\n",
        "    print(' Node accuracy: {0:.2f} %% ({1:,d}/{2:,d})'.format(\n",
        "        acc_node, result['correct_node'], result['total_node']))\n",
        "    print(' Root accuracy: {0:.2f} %% ({1:,d}/{2:,d})'.format(\n",
        "        acc_root, result['correct_root'], result['total_root']))\n",
        "            \n",
        "print('Test evaluation')\n",
        "evaluate(model, test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test evaluation\n",
            " Node accuracy: 54.49 %% (170/312)\n",
            " Root accuracy: 50.00 %% (5/10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rVhX6oQe9eIg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# (Advanced) Mini-batching in Recursive Neural Network[1]\n",
        "\n",
        "Recursive Neural Network is difficult to compute mini-batched data in parallel because of the following reasons.\n",
        "\n",
        "* Data length varies depending on samples.\n",
        "* The order of calculation for each sample is different.\n",
        "\n",
        "However, using the stack, Recursive Neural Network can perform mini batch parallel calculation."
      ]
    },
    {
      "metadata": {
        "id": "0n0Nm1wqKr_l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparation of Dataset, Iterator\n",
        "\n",
        "First, we convert the recursive calculation of Recursive Neural Network to a serial calculation using a stack.\n",
        "\n",
        "For each node of the tree structure dataset, numbers are assigned to each node in \"returning order\" as follows.\n",
        "\n",
        "<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/k/kumechann/20180519/20180519103841.png\" width=\"300\">\n",
        "\n",
        "The returning order is a procedure of numbering nodes of a tree structure. It is a procedure of attaching a smaller number to all child nodes than the parent node. If you process nodes in descending order of numbers, you can trace the child nodes before the parent node.\n"
      ]
    },
    {
      "metadata": {
        "id": "JhCaQqMrwrjj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def linearize_tree(vocab, root, xp=np):\n",
        "    # Left node indexes for all parent nodes\n",
        "    lefts = []\n",
        "    # Right node indexes for all parent nodes\n",
        "    rights = []\n",
        "    # Parent node indexes\n",
        "    dests = []\n",
        "    # All labels to predict for all parent nodes\n",
        "    labels = []\n",
        "\n",
        "    # All words of leaf nodes\n",
        "    words = []\n",
        "    # Leaf labels\n",
        "    leaf_labels = []\n",
        "\n",
        "    # Current leaf node index\n",
        "    leaf_index = [0]\n",
        "\n",
        "    def traverse_leaf(exp):\n",
        "        if len(exp) == 2:\n",
        "            label, leaf = exp\n",
        "            if leaf not in vocab:\n",
        "                vocab[leaf] = len(vocab)\n",
        "            words.append(vocab[leaf])\n",
        "            leaf_labels.append(int(label))\n",
        "            leaf_index[0] += 1\n",
        "        elif len(exp) == 3:\n",
        "            _, left, right = exp\n",
        "            traverse_leaf(left)\n",
        "            traverse_leaf(right)\n",
        "\n",
        "    traverse_leaf(root)\n",
        "\n",
        "    # Current internal node index\n",
        "    node_index = leaf_index\n",
        "    leaf_index = [0]\n",
        "\n",
        "    def traverse_node(exp):\n",
        "        if len(exp) == 2:\n",
        "            leaf_index[0] += 1\n",
        "            return leaf_index[0] - 1\n",
        "        elif len(exp) == 3:\n",
        "            label, left, right = exp\n",
        "            l = traverse_node(left)\n",
        "            r = traverse_node(right)\n",
        "\n",
        "            lefts.append(l)\n",
        "            rights.append(r)\n",
        "            dests.append(node_index[0])\n",
        "            labels.append(int(label))\n",
        "\n",
        "            node_index[0] += 1\n",
        "            return node_index[0] - 1\n",
        "\n",
        "    traverse_node(root)\n",
        "    assert len(lefts) == len(words) - 1\n",
        "\n",
        "    return {\n",
        "        'lefts': xp.array(lefts, 'i'),\n",
        "        'rights': xp.array(rights, 'i'),\n",
        "        'dests': xp.array(dests, 'i'),\n",
        "        'words': xp.array(words, 'i'),\n",
        "        'labels': xp.array(labels, 'i'),\n",
        "        'leaf_labels': xp.array(leaf_labels, 'i'),\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s2nu45Tiw0CY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "xp = cuda.cupy if gpu_id >= 0 else np\n",
        "\n",
        "vocab = {}\n",
        "\n",
        "train_data = [linearize_tree(vocab, t, xp)\n",
        "                        for t in read_corpus('trees/train.txt', max_size)]\n",
        "train_iter = chainer.iterators.SerialIterator(train_data, batchsize)\n",
        "\n",
        "validation_data = [linearize_tree(vocab, t, xp)\n",
        "                       for t in read_corpus('trees/dev.txt', max_size)]\n",
        "validation_iter = chainer.iterators.SerialIterator(\n",
        "    validation_data, batchsize, repeat=False, shuffle=False)\n",
        "\n",
        "test_data = [linearize_tree(vocab, t, xp)\n",
        "                       for t in read_corpus('trees/test.txt', max_size)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W8D3CHRKDngQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's try to display the first element of ``test_data``.\n",
        "\n",
        "``lefts`` is the index of the left node for the ``dests`` parent node, ``rights`` is the index of the right node for the ``dests`` parent node, `` dests`` is the parent node's index, `dictionary` contains word id,` `labels`` has parent node label, and` `leaf_labels`` contains dictionary of leaf node labels."
      ]
    },
    {
      "metadata": {
        "id": "UGmbQTWvDuwB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a0db76b0-12b3-44d4-e1a8-12a0c231f8f7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527407494078,
          "user_tz": -540,
          "elapsed": 400,
          "user": {
            "displayName": "Keisuke Umezawa",
            "photoUrl": "//lh3.googleusercontent.com/-KRaJUYLr9rk/AAAAAAAAAAI/AAAAAAAAABE/PM10WWYaRqk/s50-c-k-no/photo.jpg",
            "userId": "105009450683196526456"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(test_data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'lefts': array([0, 2, 4], dtype=int32), 'rights': array([1, 3, 5], dtype=int32), 'dests': array([4, 5, 6], dtype=int32), 'words': array([252,  71, 253, 254], dtype=int32), 'labels': array([3, 1, 2], dtype=int32), 'leaf_labels': array([3, 2, 1, 2], dtype=int32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TV7FdFj8Kw6d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Definition of mini-batchable models"
      ]
    },
    {
      "metadata": {
        "id": "KCBnHB50WwmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Recursive Neural Network has two operations: Operation A for computing an embedding vector for the leaf node. Operation B for computing the hidden state vector of the parent node from the hidden state vectors of the two child nodes.\n",
        "\n",
        "For each sample, we assign index to each node in returning order. If you traverse the node in return order, you will find that operation A is performed on the leaf node and operation B is performed at the other nodes.\n",
        "\n",
        "This operation can also be regarded as using a stack to scan a tree structure. A stack is a last-in, first-out data structure that allows you to do two things: a push operation to add data and a pop operation to get the last pushed data.\n",
        "\n",
        "For operation A, push the calculation result to the stack. For operation B, pop two data and push the new calculation result.\n",
        "\n",
        "When we parallelize the above operation, it is necessary to traverse nodes and perform operation A and operation B precisely because the tree structure is different for each sample. However, by using the stack,  we can calculate different tree structures by simple repeating processing. Therefore, parallelization is possible."
      ]
    },
    {
      "metadata": {
        "id": "o7NxCajVRDIY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from chainer import cuda\n",
        "from chainer.utils import type_check\n",
        "\n",
        "\n",
        "class ThinStackSet(chainer.Function):\n",
        "    \"\"\"Set values to a thin stack.\"\"\"\n",
        "\n",
        "    def check_type_forward(self, in_types):\n",
        "        type_check.expect(in_types.size() == 3)\n",
        "        s_type, i_type, v_type = in_types\n",
        "        type_check.expect(\n",
        "            s_type.dtype.kind == 'f',\n",
        "            i_type.dtype.kind == 'i',\n",
        "            s_type.dtype == v_type.dtype,\n",
        "            s_type.ndim == 3,\n",
        "            i_type.ndim == 1,\n",
        "            v_type.ndim == 2,\n",
        "            s_type.shape[0] >= i_type.shape[0],\n",
        "            i_type.shape[0] == v_type.shape[0],\n",
        "            s_type.shape[2] == v_type.shape[1],\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        xp = cuda.get_array_module(*inputs)\n",
        "        stack, indices, values = inputs\n",
        "        stack[xp.arange(len(indices)), indices] = values\n",
        "        return stack,\n",
        "\n",
        "    def backward(self, inputs, grads):\n",
        "        xp = cuda.get_array_module(*inputs)\n",
        "        _, indices, _ = inputs\n",
        "        g = grads[0]\n",
        "        gv = g[xp.arange(len(indices)), indices]\n",
        "        g[xp.arange(len(indices)), indices] = 0\n",
        "        return g, None, gv\n",
        "\n",
        "\n",
        "def thin_stack_set(s, i, x):\n",
        "    return ThinStackSet()(s, i, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8t4bR-W704up",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In addition, we use thin stack[2] instead of simple stack here.\n",
        "\n",
        "Let the sentence length be $I$ and the number of dimensions of the hidden vector be $D$, the thin stack can efficiently use the memory by using the matrix of $(2I-1) \\times D$.\n",
        "\n",
        "In a normal stack, you need $O(I^2 D)$ space computation, whereas thin stacks require $O(ID)$.\n",
        "\n",
        "It is realized by push operation ``thin_stack_set`` and pop operation ``thin_stack_get``.\n",
        "\n",
        "First of all, we define ``ThinStackSet`` and ``ThinStackGet`` which inherit ``chainer.Function``.\n",
        "\n",
        "``ThinStackSet`` is literally a function to set values on the thin stack.\n",
        "\n",
        "`` inputs`` in ``forward`` and `` backward``  can be broken down like ``stack, indices, values = inputs``.\n",
        "\n",
        "``stack`` is shared by functions by setting it as a function argument in the thin stack itself.\n",
        "\n",
        "Because ``chainer.Function`` does not have internal states inside, it handles ``stack`` externally by passing it as a function argument."
      ]
    },
    {
      "metadata": {
        "id": "s5P9sQzGCg6G",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class ThinStackGet(chainer.Function):\n",
        "\n",
        "    def check_type_forward(self, in_types):\n",
        "        type_check.expect(in_types.size() == 2)\n",
        "        s_type, i_type = in_types\n",
        "        type_check.expect(\n",
        "            s_type.dtype.kind == 'f',\n",
        "            i_type.dtype.kind == 'i',\n",
        "            s_type.ndim == 3,\n",
        "            i_type.ndim == 1,\n",
        "            s_type.shape[0] >= i_type.shape[0],\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        xp = cuda.get_array_module(*inputs)\n",
        "        stack, indices = inputs\n",
        "        return stack[xp.arange(len(indices)), indices], stack\n",
        "\n",
        "    def backward(self, inputs, grads):\n",
        "        xp = cuda.get_array_module(*inputs)\n",
        "        stack, indices = inputs\n",
        "        g, gs = grads\n",
        "        if gs is None:\n",
        "            gs = xp.zeros_like(stack)\n",
        "        if g is not None:\n",
        "            gs[xp.arange(len(indices)), indices] += g\n",
        "        return gs, None\n",
        "\n",
        "\n",
        "def thin_stack_get(s, i):\n",
        "    return ThinStackGet()(s, i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5TLGCBDazDod",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`` ThinStackGet`` is literally a function to retrieve values from the thin stack.\n",
        "\n",
        "`` inputs`` in `` forward`` and `` backward`` can be broken down like ``stack, indices = inputs``."
      ]
    },
    {
      "metadata": {
        "id": "69Dpc3TrKe1Z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class ThinStackRecursiveNet(chainer.Chain):\n",
        "\n",
        "    def __init__(self, n_vocab, n_units, n_label):\n",
        "        super(ThinStackRecursiveNet, self).__init__(\n",
        "            embed=L.EmbedID(n_vocab, n_units),\n",
        "            l=L.Linear(n_units * 2, n_units),\n",
        "            w=L.Linear(n_units, n_label))\n",
        "        self.n_units = n_units\n",
        "\n",
        "    def leaf(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "    def node(self, left, right):\n",
        "        return F.tanh(self.l(F.concat((left, right))))\n",
        "\n",
        "    def label(self, v):\n",
        "        return self.w(v)\n",
        "\n",
        "    def __call__(self, *inputs):\n",
        "        batch = len(inputs) // 6\n",
        "        lefts = inputs[0: batch]\n",
        "        rights = inputs[batch: batch * 2]\n",
        "        dests = inputs[batch * 2: batch * 3]\n",
        "        labels = inputs[batch * 3: batch * 4]\n",
        "        sequences = inputs[batch * 4: batch * 5]\n",
        "        leaf_labels = inputs[batch * 5: batch * 6]\n",
        "\n",
        "        inds = np.argsort([-len(l) for l in lefts])\n",
        "        # Sort all arrays in descending order and transpose them\n",
        "        lefts = F.transpose_sequence([lefts[i] for i in inds])\n",
        "        rights = F.transpose_sequence([rights[i] for i in inds])\n",
        "        dests = F.transpose_sequence([dests[i] for i in inds])\n",
        "        labels = F.transpose_sequence([labels[i] for i in inds])\n",
        "        sequences = F.transpose_sequence([sequences[i] for i in inds])\n",
        "        leaf_labels = F.transpose_sequence([leaf_labels[i] for i in inds])\n",
        "\n",
        "        batch = len(inds)\n",
        "        maxlen = len(sequences)\n",
        "\n",
        "        loss = 0\n",
        "        count = 0\n",
        "        correct = 0\n",
        "\n",
        "        # thin stack\n",
        "        stack = self.xp.zeros((batch, maxlen * 2, self.n_units), 'f')\n",
        "\n",
        "        # èãã¼ãã®é ãç¶æãã¯ãã«ã¨lossãè¨ç®\n",
        "        for i, (word, label) in enumerate(zip(sequences, leaf_labels)):\n",
        "            batch = word.shape[0]\n",
        "            es = self.leaf(word)\n",
        "            ds = self.xp.full((batch,), i, 'i')\n",
        "            y = self.label(es)\n",
        "            loss += F.softmax_cross_entropy(y, label, normalize=False) * batch\n",
        "            count += batch\n",
        "            predict = self.xp.argmax(y.data, axis=1)\n",
        "            correct += (predict == label.data).sum()\n",
        "\n",
        "            stack = thin_stack_set(stack, ds, es)\n",
        "\n",
        "        # ä¸­éãã¼ãã®é ãç¶æãã¯ãã«ã¨lossãè¨ç®\n",
        "        for left, right, dest, label in zip(lefts, rights, dests, labels):\n",
        "            l, stack = thin_stack_get(stack, left)\n",
        "            r, stack = thin_stack_get(stack, right)\n",
        "            o = self.node(l, r)\n",
        "            y = self.label(o)\n",
        "            batch = l.shape[0]\n",
        "            loss += F.softmax_cross_entropy(y, label, normalize=False) * batch\n",
        "            count += batch\n",
        "            predict = self.xp.argmax(y.data, axis=1)\n",
        "            correct += (predict == label.data).sum()\n",
        "\n",
        "            stack = thin_stack_set(stack, dest, o)\n",
        "\n",
        "        loss /= count\n",
        "        reporter.report({'loss': loss}, self)\n",
        "        reporter.report({'total': count}, self)\n",
        "        reporter.report({'correct': correct}, self)\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wzaoOvy-CE83",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d52ad030-d23a-4b3a-d2b8-f2ba74aeabb7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526826017145,
          "user_tz": -540,
          "elapsed": 760,
          "user": {
            "displayName": "Keisuke Umezawa",
            "photoUrl": "//lh3.googleusercontent.com/-KRaJUYLr9rk/AAAAAAAAAAI/AAAAAAAAABE/PM10WWYaRqk/s50-c-k-no/photo.jpg",
            "userId": "105009450683196526456"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = ThinStackRecursiveNet(len(vocab), n_units, n_label)\n",
        "\n",
        "if gpu_id >= 0:\n",
        "    model.to_gpu()\n",
        "    \n",
        "optimizer = chainer.optimizers.AdaGrad(0.1)\n",
        "optimizer.setup(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<chainer.optimizers.ada_grad.AdaGrad at 0x7f8a3c453710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "vP8_30gMLAjT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparation of Updater Â· Trainer and execution of training\n",
        "\n",
        "Let's train with the new model `` ThinStackRecursiveNet``. Since you can now compute mini batches in parallel, you can see that training is faster."
      ]
    },
    {
      "metadata": {
        "id": "Dmhq_uyAxNd4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "outputId": "28e94c4b-60d3-4e39-c841-2fb858cd6940",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526826052858,
          "user_tz": -540,
          "elapsed": 35667,
          "user": {
            "displayName": "Keisuke Umezawa",
            "photoUrl": "//lh3.googleusercontent.com/-KRaJUYLr9rk/AAAAAAAAAAI/AAAAAAAAABE/PM10WWYaRqk/s50-c-k-no/photo.jpg",
            "userId": "105009450683196526456"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def convert(batch, device):\n",
        "    if device is None:\n",
        "        def to_device(x):\n",
        "            return x\n",
        "    elif device < 0:\n",
        "        to_device = cuda.to_cpu\n",
        "    else:\n",
        "        def to_device(x):\n",
        "            return cuda.to_gpu(x, device, cuda.Stream.null)\n",
        "\n",
        "    return tuple(\n",
        "        [to_device(d['lefts']) for d in batch] +\n",
        "        [to_device(d['rights']) for d in batch] +\n",
        "        [to_device(d['dests']) for d in batch] +\n",
        "        [to_device(d['labels']) for d in batch] +\n",
        "        [to_device(d['words']) for d in batch] +\n",
        "        [to_device(d['leaf_labels']) for d in batch]\n",
        "    )\n",
        "  \n",
        "\n",
        "updater = chainer.training.StandardUpdater(\n",
        "    train_iter, optimizer, device=None, converter=convert)\n",
        "trainer = chainer.training.Trainer(updater, (n_epoch, 'epoch'))\n",
        "trainer.extend(\n",
        "    extensions.Evaluator(validation_iter, model, converter=convert, device=None),\n",
        "    trigger=(epoch_per_eval, 'epoch'))\n",
        "trainer.extend(extensions.LogReport())\n",
        "\n",
        "trainer.extend(extensions.MicroAverage(\n",
        "    'main/correct', 'main/total', 'main/accuracy'))\n",
        "trainer.extend(extensions.MicroAverage(\n",
        "    'validation/main/correct', 'validation/main/total',\n",
        "    'validation/main/accuracy'))\n",
        "\n",
        "trainer.extend(extensions.PrintReport(\n",
        "   ['epoch', 'main/loss', 'validation/main/loss',\n",
        "     'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
        "\n",
        "trainer.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
            "\u001b[J1           1.75582                           0.268018                                 0.772637      \n",
            "\u001b[J2           1.0503      1.52234               0.63964        0.448087                  1.74078       \n",
            "\u001b[J3           0.752925                          0.743243                                 2.52495       \n",
            "\u001b[J4           1.21727     1.46956               0.745495       0.456284                  3.49669       \n",
            "\u001b[J5           0.681582                          0.817568                                 4.24974       \n",
            "\u001b[J6           0.477964    1.5514                0.880631       0.480874                  5.22265       \n",
            "\u001b[J7           0.38437                           0.916667                                 5.98324       \n",
            "\u001b[J8           0.30405     1.68066               0.923423       0.469945                  6.94833       \n",
            "\u001b[J9           0.222884                          0.959459                                 7.69772       \n",
            "\u001b[J10          0.175159    1.79104               0.977477       0.478142                  8.67923       \n",
            "\u001b[J11          0.142888                          0.97973                                  9.43108       \n",
            "\u001b[J12          0.118272    1.87948               0.986486       0.47541                   10.4046       \n",
            "\u001b[J13          0.0991659                         0.997748                                 11.1994       \n",
            "\u001b[J14          0.0841932   1.95415               0.997748       0.478142                  12.1657       \n",
            "\u001b[J15          0.0723124                         0.997748                                 12.9141       \n",
            "\u001b[J16          0.0627568   2.01682               0.997748       0.480874                  13.8787       \n",
            "\u001b[J17          0.0549726                         1                                        14.6336       \n",
            "\u001b[J18          0.04857     2.07107               1              0.478142                  15.6061       \n",
            "\u001b[J19          0.0432675                         1                                        16.3584       \n",
            "\u001b[J20          0.0388425   2.1181                1              0.480874                  17.3297       \n",
            "\u001b[J21          0.035117                          1                                        18.0761       \n",
            "\u001b[J22          0.0319522   2.15905               1              0.478142                  19.0487       \n",
            "\u001b[J23          0.0292416                         1                                        19.8416       \n",
            "\u001b[J24          0.0269031   2.1951                1              0.480874                  20.8083       \n",
            "\u001b[J25          0.0248729                         1                                        21.5566       \n",
            "\u001b[J26          0.0231      2.22721               1              0.483607                  22.5304       \n",
            "\u001b[J27          0.0215427                         1                                        23.2878       \n",
            "\u001b[J28          0.0201669   2.25614               1              0.486339                  24.2565       \n",
            "\u001b[J29          0.018944                          1                                        25.0171       \n",
            "\u001b[J30          0.017851    2.28247               1              0.480874                  26.0063       \n",
            "\u001b[J31          0.0168687                         1                                        26.7633       \n",
            "\u001b[J32          0.0159814   2.30664               1              0.483607                  27.7331       \n",
            "\u001b[J33          0.0151763                         1                                        28.5342       \n",
            "\u001b[J34          0.0144427   2.32898               1              0.483607                  29.5039       \n",
            "\u001b[J35          0.0137716                         1                                        30.257        \n",
            "\u001b[J36          0.0131555   2.34976               1              0.483607                  31.2306       \n",
            "\u001b[J37          0.0125881                         1                                        31.9842       \n",
            "\u001b[J38          0.0120638   2.3692                1              0.483607                  32.9617       \n",
            "\u001b[J39          0.0115783                         1                                        33.7175       \n",
            "\u001b[J40          0.0111272   2.38747               1              0.483607                  34.6946       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kIYQVBOAJ4eg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It got much faster!"
      ]
    },
    {
      "metadata": {
        "id": "_UBmkq3xYmSX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "[1] æ·±å±¤å­¦ç¿ã«ããèªç¶è¨èªå¦ç (æ©æ¢°å­¦ç¿ãã­ãã§ãã·ã§ãã«ã·ãªã¼ãº)\n",
        "\n",
        "[2] [A Fast Unified Model for Parsing and Sentence Understanding](http://nlp.stanford.edu/pubs/bowman2016spinn.pdf)"
      ]
    }
  ]
}